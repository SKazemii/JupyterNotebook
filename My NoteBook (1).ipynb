{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": false,
      "skip_h1_title": false,
      "base_numbering": 1,
      "title_cell": "Table of Contents",
      "title_sidebar": "Content",
      "toc_cell": true,
      "toc_position": {
        "height": "304px",
        "width": "347px",
        "left": "307px",
        "top": "697px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "window_display": false,
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "library": "var_list.py",
          "delete_cmd_prefix": "del ",
          "delete_cmd_postfix": "",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "library": "var_list.r",
          "delete_cmd_prefix": "rm(",
          "delete_cmd_postfix": ") ",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ]
    },
    "colab": {
      "name": "My NoteBook (1).ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "toc": true,
        "id": "bqytGdkbdOj7"
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#The-Project-Process\" data-toc-modified-id=\"The-Project-Process-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>The Project Process</a></span></li><li><span><a href=\"#Dividing-Data-Sets\" data-toc-modified-id=\"Dividing-Data-Sets-0.2\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span>Dividing Data Sets</a></span></li><li><span><a href=\"#What-Is-the-Difference-Between-Batch-and-Epoch?\" data-toc-modified-id=\"What-Is-the-Difference-Between-Batch-and-Epoch?-0.3\"><span class=\"toc-item-num\">0.3&nbsp;&nbsp;</span>What Is the Difference Between Batch and Epoch?</a></span></li></ul></li><li><span><a href=\"#Jupyter-NoteBook--\" data-toc-modified-id=\"Jupyter-NoteBook---1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Jupyter NoteBook  </a></span><ul class=\"toc-item\"><li><span><a href=\"#For-Making-Headings:\" data-toc-modified-id=\"For-Making-Headings:-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>For Making Headings:</a></span></li><li><span><a href=\"#For-Making-Blockquotes\" data-toc-modified-id=\"For-Making-Blockquotes-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>For Making Blockquotes</a></span></li><li><span><a href=\"#For-Making-Code-section\" data-toc-modified-id=\"For-Making-Code-section-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>For Making Code section</a></span></li><li><span><a href=\"#For-Making-Mathematical-Symbol\" data-toc-modified-id=\"For-Making-Mathematical-Symbol-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>For Making Mathematical Symbol</a></span></li><li><span><a href=\"#For-Making-Line-Break\" data-toc-modified-id=\"For-Making-Line-Break-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>For Making Line Break</a></span></li><li><span><a href=\"#For-Making-Bold-and-Italic-Text\" data-toc-modified-id=\"For-Making-Bold-and-Italic-Text-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>For Making Bold and Italic Text</a></span></li><li><span><a href=\"#For-Making-Horizontal-Lines\" data-toc-modified-id=\"For-Making-Horizontal-Lines-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>For Making Horizontal Lines</a></span></li><li><span><a href=\"#For-Making-Ordered-List\" data-toc-modified-id=\"For-Making-Ordered-List-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>For Making Ordered List</a></span></li><li><span><a href=\"#For-Making-Unordered-List\" data-toc-modified-id=\"For-Making-Unordered-List-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>For Making Unordered List</a></span></li><li><span><a href=\"#For-Making-Internal-and-External-Link\" data-toc-modified-id=\"For-Making-Internal-and-External-Link-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>For Making Internal and External Link</a></span></li><li><span><a href=\"#For-Making-Image\" data-toc-modified-id=\"For-Making-Image-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>For Making Image</a></span></li></ul></li><li><span><a href=\"#Pandas-library\" data-toc-modified-id=\"Pandas-library-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Pandas library</a></span><ul class=\"toc-item\"><li><span><a href=\"#Install-Package\" data-toc-modified-id=\"Install-Package-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Install Package</a></span></li><li><span><a href=\"#Pandas-Series-and-DataFrame-representation\" data-toc-modified-id=\"Pandas-Series-and-DataFrame-representation-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Pandas Series and DataFrame representation</a></span></li><li><span><a href=\"#Useful-Methods\" data-toc-modified-id=\"Useful-Methods-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Useful Methods</a></span></li><li><span><a href=\"#Import-and-Export-in-Pandas\" data-toc-modified-id=\"Import-and-Export-in-Pandas-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Import and Export in Pandas</a></span></li><li><span><a href=\"#Select-a-subset-of-a-DataFrame\" data-toc-modified-id=\"Select-a-subset-of-a-DataFrame-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Select a subset of a DataFrame</a></span></li></ul></li><li><span><a href=\"#Machine-Learning-(sci-kit-learn)\" data-toc-modified-id=\"Machine-Learning-(sci-kit-learn)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Machine Learning (sci-kit learn)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importing-Libs\" data-toc-modified-id=\"Importing-Libs-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Importing Libs</a></span></li><li><span><a href=\"#Make-a-dataset\" data-toc-modified-id=\"Make-a-dataset-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Make a dataset</a></span></li><li><span><a href=\"#MLP-Classifier\" data-toc-modified-id=\"MLP-Classifier-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>MLP Classifier</a></span></li><li><span><a href=\"#K-Neighbors-Classifier\" data-toc-modified-id=\"K-Neighbors-Classifier-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>K-Neighbors Classifier</a></span></li><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Logistic Regression</a></span></li><li><span><a href=\"#Linear-Regression\" data-toc-modified-id=\"Linear-Regression-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Linear Regression</a></span></li><li><span><a href=\"#K-Neighbors-Regressor\" data-toc-modified-id=\"K-Neighbors-Regressor-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>K-Neighbors Regressor</a></span></li><li><span><a href=\"#Principal-Component-Analysis-(PCA)\" data-toc-modified-id=\"Principal-Component-Analysis-(PCA)-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Principal Component Analysis (PCA)</a></span></li><li><span><a href=\"#Unsupervised-Learning:-K-Means-Clustering\" data-toc-modified-id=\"Unsupervised-Learning:-K-Means-Clustering-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span>Unsupervised Learning: K-Means Clustering</a></span></li></ul></li><li><span><a href=\"#Computer-Vision\" data-toc-modified-id=\"Computer-Vision-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Computer Vision</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-Perspective-Transform\" data-toc-modified-id=\"Get-Perspective-Transform-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Get Perspective Transform</a></span></li><li><span><a href=\"#Get-Affine-Transform\" data-toc-modified-id=\"Get-Affine-Transform-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Get Affine Transform</a></span></li><li><span><a href=\"#Flip-the-webcam-video\" data-toc-modified-id=\"Flip-the-webcam-video-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Flip the webcam video</a></span></li><li><span><a href=\"#Split-the-Chanel-of-webcam-video\" data-toc-modified-id=\"Split-the-Chanel-of-webcam-video-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Split the Chanel of webcam video</a></span></li><li><span><a href=\"#Make-binary-image\" data-toc-modified-id=\"Make-binary-image-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Make binary image</a></span></li><li><span><a href=\"#Flip-the-webcam-video\" data-toc-modified-id=\"Flip-the-webcam-video-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Flip the webcam video</a></span></li><li><span><a href=\"#Flip-the-webcam-video\" data-toc-modified-id=\"Flip-the-webcam-video-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Flip the webcam video</a></span></li></ul></li><li><span><a href=\"#Deep-Learning-(PyTorch)\" data-toc-modified-id=\"Deep-Learning-(PyTorch)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Deep Learning (PyTorch)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Image-Classification\" data-toc-modified-id=\"Image-Classification-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Image Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-Library\" data-toc-modified-id=\"Import-Library-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Import Library</a></span></li><li><span><a href=\"#Import-Data\" data-toc-modified-id=\"Import-Data-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Import Data</a></span></li></ul></li><li><span><a href=\"#Face-Recognition\" data-toc-modified-id=\"Face-Recognition-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Face Recognition</a></span></li><li><span><a href=\"#Object-Detection\" data-toc-modified-id=\"Object-Detection-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Object Detection</a></span></li></ul></li><li><span><a href=\"#Deep-Learning-(Keras)\" data-toc-modified-id=\"Deep-Learning-(Keras)-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Deep Learning (Keras)</a></span><ul class=\"toc-item\"><li><span><a href=\"#How-to-install-Keras\" data-toc-modified-id=\"How-to-install-Keras-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>How to install Keras</a></span><ul class=\"toc-item\"><li><span><a href=\"#GPU-version:\" data-toc-modified-id=\"GPU-version:-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>GPU version:</a></span></li><li><span><a href=\"#CPU-version:\" data-toc-modified-id=\"CPU-version:-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>CPU version:</a></span></li></ul></li><li><span><a href=\"#Tree-Structure-of-Keras\" data-toc-modified-id=\"Tree-Structure-of-Keras-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Tree Structure of Keras</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loss-function\" data-toc-modified-id=\"Loss-function-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Loss function</a></span></li></ul></li><li><span><a href=\"#Multi-layer-Neural-Network\" data-toc-modified-id=\"Multi-layer-Neural-Network-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Multi-layer Neural Network</a></span></li><li><span><a href=\"#Convolutional-Neural-Networks-(CNN)\" data-toc-modified-id=\"Convolutional-Neural-Networks-(CNN)-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Convolutional Neural Networks (CNN)</a></span></li><li><span><a href=\"#Data-Generator\" data-toc-modified-id=\"Data-Generator-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Data Generator</a></span></li></ul></li><li><span><a href=\"#Telegram-Bot\" data-toc-modified-id=\"Telegram-Bot-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Telegram Bot</a></span><ul class=\"toc-item\"><li><span><a href=\"#How-to-install-Telegram-Bot-API\" data-toc-modified-id=\"How-to-install-Telegram-Bot-API-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>How to install Telegram Bot API</a></span></li><li><span><a href=\"#GET-Method\" data-toc-modified-id=\"GET-Method-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>GET Method</a></span></li><li><span><a href=\"#WEBHOOK-Method\" data-toc-modified-id=\"WEBHOOK-Method-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>WEBHOOK Method</a></span></li></ul></li><li><span><a href=\"#Block-chain\" data-toc-modified-id=\"Block-chain-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Block chain</a></span></li></ul></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K2H3r4ydOj8"
      },
      "source": [
        "<div class=\"alert alert-info\" style=\"color:blue;text-align:center;\">\n",
        "    <font size=\"9\" color=\"blue\" ><b>My Notebook in Python 3<b></font>\n",
        "    <hr>\n",
        "    <font size=\"5\" color=\"blue\" ><i>Saeed Kazemi<i></font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDQ-sp1cdOj9"
      },
      "source": [
        "## The Project Process "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjlO5kSedOj-"
      },
      "source": [
        "- Prepare Data\n",
        "\n",
        "    --Extract Date\n",
        "    \n",
        "    --Transform\n",
        "    \n",
        "    --Load\n",
        "        -Dataset class     (torch.utils.data.Dataset   )\n",
        "        -DataLoader class  (torch.utils.data.DataLoader)\n",
        "- Build the Model\n",
        "- Train the Model\n",
        "- Analyze the Model's results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qMZS64cJdOj_",
        "outputId": "c549c85c-c2d4-4236-e05c-ba7698a16c12"
      },
      "source": [
        "!pip install torchvision\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (0.4.2)\n",
            "Requirement already satisfied: numpy in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from torchvision) (1.16.2)\n",
            "Requirement already satisfied: six in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: torch in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from torchvision) (1.3.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from torchvision) (5.3.0)\n",
            "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "HnyAdmbCdOkE"
      },
      "source": [
        "import torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwCd4EGFdOkI"
      },
      "source": [
        "## Dividing Data Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snggbTgNdOkJ"
      },
      "source": [
        "Dividing Data Sets in to four subsets:\n",
        "<ol>\n",
        "<li>train set: learning the parameters of the model (about 70%).</li>\n",
        "<li>valid set: learning hyper-parameters (about 15%).</li>\n",
        "<li>test  set: only for final test (about 15%).</li>\n",
        "<li>development set: only for developing our model that include train, valid and test (about 5%).</li>\n",
        "</ol>\n",
        "\n",
        "first we find our model by development set then train hyper-parameters by using valid set\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "id": "skepu8rNdOkJ"
      },
      "source": [
        "## What Is the Difference Between Batch and Epoch?\n",
        "\n",
        "\n",
        "Assume you have a dataset with 200 samples (rows of data) and you choose a batch size of 5 and 1,000 epochs.\n",
        "\n",
        "This means that the dataset will be divided into 40 batches, each with five samples. The model weights will be updated after each batch of five samples.\n",
        "\n",
        "This also means that one epoch will involve 40 batches or 40 updates to the model.\n",
        "\n",
        "With 1,000 epochs, the model will be exposed to or pass through the whole dataset 1,000 times. That is a total of 40,000 batches during the entire training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss63RiNAdOkK"
      },
      "source": [
        "![subsets-of-ai.png](attachment:subsets-of-ai.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71Cf4AWRdOkL"
      },
      "source": [
        "<h1 style=\"color:blue;text-align:center;\">Jupyter NoteBook  </h1>\n",
        "## For Making Headings:\n",
        "<ol>\n",
        "<li>#(Header 1, title) or &lt;h1&gt;Header 1,title&lt;h1&gt;</li>\n",
        "<li>##(Header 2, major headings) or &lt;h2&gt;Header 1,title&lt;h2&gt;</li>\n",
        "<li>###(Header 3, subheadings)or &lt;h3&gt;Header 1,title&lt;h3&gt;</li>\n",
        "<li>####(Header 4) or &lt;h4&gt;Header 1,title&lt;h4&gt;</li>\n",
        "<li>####(Header 5) or &lt;h5&gt;Header 1,title&lt;h5&gt;</li>\n",
        "<li>#####(Header 6) or &lt;h6&gt;Header 1,title&lt;h6&gt;</li>\n",
        "</ol>\n",
        "    \n",
        "\n",
        "## For Making Blockquotes\n",
        ">This is good \" > text or &lt;blockquote&gt;text&lt;/blockquote&gt;\"\n",
        "\n",
        "## For Making Code section\n",
        "&lt;code&gt;code goes here&lt;code&gt; or \" ` here ` \"\n",
        "\n",
        "## For Making Mathematical Symbol\n",
        "` $ mathematical expression goes here $`\n",
        "\n",
        "## For Making Line Break\n",
        "&lt;br&gt;\n",
        "\n",
        "## For Making Bold and Italic Text\n",
        "&lt;b&gt;This is bold text &lt;b&gt;  <br>\n",
        "&lt;i&gt;This is italic text &lt;i&gt;\n",
        "\n",
        "## For Making Horizontal Lines\n",
        "'---' three hyphens or Markup tags &lt;hr&gt;\n",
        "<hr>\n",
        "\n",
        "## For Making Ordered List\n",
        "<p>&lt;ol&gt;<br>\n",
        "    &lt;li&gt;Fish&lt;/li&gt;<br>\n",
        "    &lt;li&gt;Eggs&lt;/li&gt;<br>\n",
        "    &lt;li&gt;Cheese&lt;/li&gt;<br>\n",
        "&lt;/ol&gt;</p>\n",
        "\n",
        "## For Making Unordered List\n",
        "<p>&lt;ul&gt;<br>\n",
        "    &lt;li&gt;Fish&lt;/li&gt;<br>\n",
        "    &lt;li&gt;Eggs&lt;/li&gt;<br>\n",
        "    &lt;li&gt;Cheese&lt;/li&gt;<br>\n",
        "&lt;/ul&gt;</p>\n",
        "\n",
        "## For Making Internal and External Link\n",
        "<p>&lt;a id = \"division_ID\"text goes here&gt;&lt;/a&gt;<br></p>\n",
        "<p>&lt;a&gt; href=\"<a href=\"https://www.google.com\">https://www.google.com</a>\" &gt;Link to Google&lt;/a&gt;</p>\n",
        "\n",
        "## For Making Image\n",
        "<p> &lt;img&gt; &lt;src=\"<a href=\"https://i.imgur.com/WWrydEh.png\">https://i.imgur.com/WWrydEh.png</a>\" width =\"500\" height=500 &gt;<br></p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsDWOKCmdOkL"
      },
      "source": [
        "![wczdixdckwpf.jpeg](attachment:wczdixdckwpf.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9hiiG8wdOkM"
      },
      "source": [
        "<h1 align=\"center\" style=\"color:blue;\">Pandas library</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "id": "yryaX8d5dOkM"
      },
      "source": [
        "The two primary data structures of pandas, Series (1-dimensional) and DataFrame (2-dimensional), handle the vast majority of typical use cases in finance, statistics, social science, and many areas of engineering. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB-XlCuBdOkN"
      },
      "source": [
        "> **Series**: no axis argument needed\n",
        "\n",
        "> **DataFrame**: “index” (axis=0, default), “columns” (axis=1)\n",
        "\n",
        "**Each column in a DataFrame is a Series**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd3Zbq3vdOkO"
      },
      "source": [
        "![01_table_dataframe.svg](attachment:01_table_dataframe.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf45kPtBdOkO"
      },
      "source": [
        "![01_table_series.svg](attachment:01_table_series.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqT9BoPYdOkP"
      },
      "source": [
        "## Install Package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "frLmmMLedOkQ"
      },
      "source": [
        "#!pip install pandas\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oBcYwVbdOkT"
      },
      "source": [
        "## Pandas Series and DataFrame representation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hTSDyeW7dOkU"
      },
      "source": [
        "# Create Series\n",
        "s1 = pd.Series([1,2,3,4,5,5,3,4,3], index=['a','b','c','d','e','f','g','h','i'])\n",
        "\n",
        "arr=[1,2,3,4,5,5,3,4,3]\n",
        "s2 = pd.Series(arr)\n",
        "\n",
        "dic={\"a\":1,\"b\":2,\"c\":3,\"d\":14}\n",
        "s3 = pd.Series(dic)\n",
        "\n",
        "\n",
        "\n",
        "# Create DataFtame \n",
        "import numpy as np\n",
        "dates = pd.date_range('today', periods=5)\n",
        "num_arr = np.random.randn(5,3)\n",
        "columns = ['A', 'B', 'C']\n",
        "\n",
        "df1 = pd.DataFrame(num_arr,index=dates, columns=columns)\n",
        "df1.dtypes\n",
        "\n",
        "\n",
        "mydict = [{'a': 1, 'b': 2, 'c': 3, 'd': 4},\n",
        "          {'a': 100, 'b': 200, 'c': 300, 'd': 400}, \n",
        "          {'a': 1000, 'b': 2000, 'c': 3000, 'd': 4000 }]\n",
        "df2 = pd.DataFrame(mydict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSGiw96SdOkY"
      },
      "source": [
        "## Useful Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LfY0w6okdOkY"
      },
      "source": [
        "s4 = s1.append(s3)\n",
        "\n",
        "s3.drop(\"a\", axis=0)\n",
        "\n",
        "s1.add(s3)\n",
        "s1.sub(s3)\n",
        "s1.mul(s3)\n",
        "s1.div(s3)\n",
        "s1.min()\n",
        "s1.max()\n",
        "s1.median()\n",
        "s1.sort_index()\n",
        "s1.sort_values()\n",
        "\n",
        "\n",
        "\n",
        "df1.head(3)\n",
        "df1.tail(2)\n",
        "\n",
        "\n",
        "df1.index\n",
        "df1.columns\n",
        "df1.values\n",
        "\n",
        "df1.describe()\n",
        "df2 = df1.T\n",
        "\n",
        "df1.sort_values(by='A')\n",
        "\n",
        "\n",
        "df3 = df2.copy()\n",
        "df2.isnull()\n",
        "\n",
        "df2[['b']].mean()\n",
        "df2.mean()\n",
        "df2.dropna()#Drop the rows where at least one element is missing.\n",
        "df2.fillna(4)\n",
        "\n",
        "pd.merge(df1,df2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn46nXPZdOkb"
      },
      "source": [
        "## Import and Export in Pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5jLLaeDdOkc"
      },
      "source": [
        "![02_io_readwrite.svg](attachment:02_io_readwrite.svg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RVte13V-dOkc"
      },
      "source": [
        "#df2.to_csv('namefile.csv')\n",
        "#pd.read_csv('namefile.csv')\n",
        "#df2.to_excel('namefile.xlsx', sheet_name= 'sheet1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ3rKrRsdOkj"
      },
      "source": [
        "## Select a subset of a DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ezNNY11MdOkk"
      },
      "source": [
        "df1[['A', 'B']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiwxLI0DdOko"
      },
      "source": [
        "![03_subset_columns.svg](attachment:03_subset_columns.svg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "j_eC_9tqdOkq"
      },
      "source": [
        "above_05 = df1[df1[\"A\"] > .5]\n",
        "\n",
        "\n",
        "Between02_03 = df1[df1[\"A\"].isin([0.2, 0.3])\n",
        "\n",
        "                   \n",
        "df1[1:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6rU19FKdOku"
      },
      "source": [
        "![03_subset_rows.svg](attachment:03_subset_rows.svg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "J5Ydvvk-dOku"
      },
      "source": [
        "df1.iloc[2:3, 1:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUYeFL84dOky"
      },
      "source": [
        "![03_subset_columns_rows1.svg](attachment:03_subset_columns_rows1.svg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GQmgC29zdOky"
      },
      "source": [
        "df2.iloc[1]\n",
        "\n",
        "\n",
        "df2.iloc[[0, 1], [1,2]]\n",
        "\n",
        "\n",
        "df2.loc[1,'a']= np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "jwO_eU2cdOk2"
      },
      "source": [
        "<h1 align=\"center\" style=\"color:blue;\">Machine Learning (sci-kit learn)</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBd71v3RdOk2"
      },
      "source": [
        "<h2>Importing Libs</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tEtp2n1QdOk3"
      },
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install opencv_python\n",
        "#!pip --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "XKWB-3URdOk5"
      },
      "source": [
        "import sklearn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import cv2\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "# use seaborn plotting style defaults\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhCmh4VodOk8"
      },
      "source": [
        "<h2>Make a dataset</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AgFlXeRdOk8"
      },
      "source": [
        "[Scikit-learn makes available a host of datasets for testing learning algorithms](http://scikit-learn.org/stable/datasets/#dataset-loading-utilities).\n",
        "They come in three flavors:\n",
        "\n",
        "- **Packaged Data:** these small datasets are packaged with the scikit-learn installation,\n",
        "  and can be downloaded using the tools in ``sklearn.datasets.load_*``\n",
        "- **Downloadable Data:** these larger datasets are available for download, and scikit-learn\n",
        "  includes tools which streamline this process.  These tools can be found in\n",
        "  ``sklearn.datasets.fetch_*``\n",
        "- **Generated Data:** there are several datasets which are generated from models based on a\n",
        "  random seed.  These are available in the ``sklearn.datasets.make_*``\n",
        "\n",
        "You can explore the available dataset loaders, fetchers, and generators using IPython's\n",
        "tab-completion functionality.  After importing the ``datasets`` submodule from ``sklearn``,\n",
        "type\n",
        "\n",
        "    datasets.load_<TAB>\n",
        "\n",
        "or\n",
        "\n",
        "    datasets.fetch_<TAB>\n",
        "\n",
        "or\n",
        "\n",
        "    datasets.make_<TAB>\n",
        "\n",
        "to see a list of available functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5upyrhRdOk9"
      },
      "source": [
        "<h2>MLP Classifier</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8W7pBlThdOk-"
      },
      "source": [
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "\n",
        "faces = fetch_olivetti_faces()\n",
        "\n",
        "print(faces.keys())\n",
        "\n",
        "# set up the figure\n",
        "fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "# plot the faces:\n",
        "for i in range(64):\n",
        "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
        "    ax.imshow(faces.images[i], cmap=plt.cm.bone, interpolation='nearest')\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# renaming data to simplify code\n",
        "X, y = faces.data, faces.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "clf = MLPClassifier()\n",
        "\n",
        "clf.fit(X, y)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"accuray = %.2f\" % accuracy_score(y_test, y_pred))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDNyUrkLdOlA"
      },
      "source": [
        "<h2>K-Neighbors Classifier</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Hd_xDieNdOlB"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "clf = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.5, \n",
        "                                                    random_state=123,\n",
        "                                                    stratify=y)\n",
        "\n",
        "clf.fit(X_train, y_train)      # train\n",
        "y_pred = clf.predict(X_test)   # test\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy on test data = %.2f\" % (test_accuracy * 100) + \" %\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw7CjSfldOlE"
      },
      "source": [
        "<h2>Logistic Regression</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "geKTCMyHdOlE"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.5, \n",
        "                                                    random_state=123,\n",
        "                                                    stratify=y)\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression()\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy on test data = %.2f\" % (test_accuracy * 100) + \" %\")\n",
        "\n",
        "print(clf.coef_)\n",
        "print(clf.intercept_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt8ZiZ5OdOlH"
      },
      "source": [
        "<h2>Linear Regression</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JBFM9zg0dOlH"
      },
      "source": [
        "x = np.linspace(-3, 3, 100)\n",
        "print(x[:10])\n",
        "\n",
        "rng = np.random.RandomState(42)\n",
        "y = np.sin(4 * x) + x + rng.uniform(size=len(x))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(x, y, 'o');\n",
        "\n",
        "print('Before: ', x.shape)\n",
        "X = x[:, np.newaxis]\n",
        "print('After: ', X.shape)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "print('Weight coefficients: ', regressor.coef_)\n",
        "print('y-axis intercept: ', regressor.intercept_)\n",
        "\n",
        "min_pt = X.min() * regressor.coef_[0] + regressor.intercept_\n",
        "max_pt = X.max() * regressor.coef_[0] + regressor.intercept_\n",
        "\n",
        "plt.plot([X.min(), X.max()], [min_pt, max_pt])\n",
        "plt.plot(X_train, y_train, 'o');\n",
        "\n",
        "y_pred_test = regressor.predict(X_test)\n",
        "\n",
        "plt.plot(X_test, y_test, 'o', label=\"data\")\n",
        "plt.plot(X_test, y_pred_test, 'o', label=\"prediction\")\n",
        "plt.plot([X.min(), X.max()], [min_pt, max_pt], label='fit')\n",
        "plt.legend(loc='best');\n",
        "\n",
        "regressor.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMpYWLqjdOlJ"
      },
      "source": [
        "<h2>K-Neighbors Regressor</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uu8Pk279dOlK"
      },
      "source": [
        "x = np.linspace(-3, 3, 100)\n",
        "print(x[:10])\n",
        "\n",
        "rng = np.random.RandomState(42)\n",
        "y = np.sin(4 * x) + x + rng.uniform(size=len(x))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(x, y, 'o');\n",
        "\n",
        "print('Before: ', x.shape)\n",
        "X = x[:, np.newaxis]\n",
        "print('After: ', X.shape)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "kneighbor_regression = KNeighborsRegressor(n_neighbors=1)\n",
        "kneighbor_regression.fit(X_train, y_train)\n",
        "\n",
        "y_pred_train = kneighbor_regression.predict(X_train)\n",
        "\n",
        "plt.plot(X_train, y_train, 'o', label=\"data\", markersize=10)\n",
        "plt.plot(X_train, y_pred_train, 's', label=\"prediction\", markersize=4)\n",
        "plt.legend(loc='best');\n",
        "\n",
        "y_pred_test = kneighbor_regression.predict(X_test)\n",
        "\n",
        "plt.plot(X_test, y_test, 'o', label=\"data\", markersize=8)\n",
        "plt.plot(X_test, y_pred_test, 's', label=\"prediction\", markersize=4)\n",
        "plt.legend(loc='best');\n",
        "\n",
        "print(kneighbor_regression.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jOOElq_dOlM"
      },
      "source": [
        "<h2>Principal Component Analysis (PCA)</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "913PbHcpdOlM"
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "print(X.shape)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)  # project from 64 to 2 dimensions\n",
        "X_proj = pca.fit_transform(X)\n",
        "print(X.shape)\n",
        "print(X_proj.shape)\n",
        "\n",
        "plt.figure(figsize=(10.0, 8.0))\n",
        "plt.scatter(X_proj[:, 0], X_proj[:, 1], \n",
        "            c=y, edgecolor='none', alpha=0.5,\n",
        "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
        "plt.colorbar();\n",
        "\n",
        "#Choosing the Number of Components\n",
        "sns.set()\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "pca = PCA().fit(X)  # Notice\n",
        "\n",
        "plt.bar(range(1, len(pca.explained_variance_ratio_) + 1),\n",
        "        pca.explained_variance_ratio_,\n",
        "        alpha=0.5,\n",
        "        align='center')\n",
        "\n",
        "plt.step(range(1, len(pca.explained_variance_ratio_) + 1),\n",
        "         np.cumsum(pca.explained_variance_ratio_),\n",
        "         where='mid')\n",
        "\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance');\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyeRrplodOlQ"
      },
      "source": [
        "<h2>Unsupervised Learning: K-Means Clustering</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "niKbyKYAdOlQ"
      },
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, y = make_blobs(random_state=42)\n",
        "print(X.shape)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1]);\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "\n",
        "labels = kmeans.fit_predict(X)\n",
        "print(labels)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], cmap=plt.cm.cool, c=labels);\n",
        "\n",
        "distortions = []\n",
        "for i in range(1, 11):\n",
        "    km = KMeans(n_clusters=i, random_state=0)\n",
        "    km.fit(X)\n",
        "    distortions.append(km.inertia_)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, 11), distortions, marker='o')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Distortion')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK4Ka4RFdOlU"
      },
      "source": [
        "![cluster_comparison.png](attachment:cluster_comparison.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mp_LN3VUdOlU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YL8NP4hXdOlY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RrpJ_P1idOlb"
      },
      "source": [
        "<h2 style=\"color:red;\">Unsupervised Learning: Clustering</h2>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "MqzB2ZEudOle"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ZBCiji40dOlg"
      },
      "source": [
        "<h2 style=\"color:red;\">Unsupervised Learning: Clustering</h2>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "id": "XFVFJyrVdOll"
      },
      "source": [
        "<h1 align=\"center\" style=\"color:blue;\">Computer Vision</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WRva8kundOlm"
      },
      "source": [
        "\n",
        "#!pip install --upgrade pip\n",
        "#!pip install opencv_python\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4feozy9dOlo"
      },
      "source": [
        "<h2>Get Perspective Transform</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PelsGm4RdOlp"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "image = cv2.imread('images/dog.jpg')\n",
        "\n",
        "cv2.imshow('Original', image)\n",
        "cv2.waitKey(0)\n",
        "\n",
        "# Cordinates of the 4 corners of the original image\n",
        "points_A = np.float32([[320,15], [700,215], [85,610], [530,780]])\n",
        "\n",
        "# Cordinates of the 4 corners of the desired output\n",
        "# We use a ratio of an A4 Paper 1 : 1.41\n",
        "points_B = np.float32([[0,0], [420,0], [0,594], [420,594]])\n",
        " \n",
        "# Use the two sets of four points to compute \n",
        "# the Perspective Transformation matrix, M    \n",
        "M = cv2.getPerspectiveTransform(points_A, points_B)\n",
        " \n",
        "warped = cv2.warpPerspective(image, M, (420,594))\n",
        "\n",
        "cv2.imshow('warpPerspective', warped)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNAvSJ0vdOlr"
      },
      "source": [
        "<h2>Get Affine Transform</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Xg1aRBWidOls"
      },
      "source": [
        "image = cv2.imread('scan.jpg')\n",
        "rows,cols,ch = image.shape\n",
        " \n",
        "cv2.imshow('Original', image)\n",
        "cv2.waitKey(0)\n",
        " \n",
        "# Cordinates of the 4 corners of the original image\n",
        "points_A = np.float32([[320,15], [700,215], [85,610]])\n",
        " \n",
        "# Cordinates of the 4 corners of the desired output\n",
        "# We use a ratio of an A4 Paper 1 : 1.41\n",
        "points_B = np.float32([[0,0], [420,0], [0,594]])\n",
        " \n",
        "# Use the two sets of four points to compute \n",
        "# the Perspective Transformation matrix, M    \n",
        "M = cv2.getAffineTransform(points_A, points_B)\n",
        " \n",
        "warped = cv2.warpAffine(image, M, (cols, rows))\n",
        "print(ch)\n",
        "cv2.imshow('warpPerspective', warped)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUJgSVandOlu"
      },
      "source": [
        "<h2>Flip the webcam video</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xHIUVgYndOlv"
      },
      "source": [
        "cap = cv2.VideoCapture(0)\n",
        " \n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    flipped = cv2.flip(frame, 0)\n",
        "    cv2.imshow('Webcam', flipped)\n",
        "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
        "        break\n",
        "        \n",
        "# Release camera and close windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fija3fzsdOlx"
      },
      "source": [
        "<h2>Split the Chanel of webcam video</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "vzPuwvGCdOlx"
      },
      "source": [
        "cap = cv2.VideoCapture(1)\n",
        " \n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    type(frame)\n",
        "    height, width, _ = frame.shape\n",
        "    zeros = np.zeros([height,width], dtype = \"uint8\")\n",
        "    B, G, R = cv2.split(frame)\n",
        "    image_green = cv2.merge([zeros, G, zeros])\n",
        "    image_red = cv2.merge([zeros, zeros, R])\n",
        "    \n",
        "    cv2.imshow('Webcam Green', image_green)\n",
        "    cv2.imshow('Webcam Red', image_red)\n",
        "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
        "        break\n",
        "        \n",
        "# Release camera and close windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": false,
        "id": "8Dx7LFDEdOl0"
      },
      "source": [
        "<h2>Make binary image</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DKW0XI4JdOl0"
      },
      "source": [
        "# Load our image as greyscale \n",
        "image = cv2.imread('./images/dog.jpg')\n",
        " \n",
        "new_image = np.copy(image)\n",
        "new_image[new_image <  127] = 0\n",
        "new_image[new_image >= 127] = 255\n",
        " \n",
        "cv2.imshow('Original', image)\n",
        "cv2.imshow('After Threshhold, Binary Image', new_image)\n",
        "cv2.waitKey(0) \n",
        "    \n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "id": "SqGoPiBLdOl3"
      },
      "source": [
        "<h2>Flip the webcam video</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wCrUnJopdOl3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "id": "0Qvk2_8JdOl6"
      },
      "source": [
        "<h2>Flip the webcam video</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFgcIbAWdOl6"
      },
      "source": [
        "<h1 align=\"center\" style=\"color:blue;\">Deep Learning (PyTorch)</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "id": "drlRq3gpdOl6"
      },
      "source": [
        "- Four key components of any ML system (in PyTorch):\n",
        " - Data (Images)\n",
        " - Model (CNN)\n",
        " - Loss (Cross Entropy)\n",
        " - Optimization (SGD, Adam, ..)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKgYyteudOl7"
      },
      "source": [
        "- Overfit\n",
        "- Data augmentation\n",
        "- Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "id": "Te8XONFYdOl7"
      },
      "source": [
        "## Image Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWeCaxDldOl9"
      },
      "source": [
        "Can classify an image according to its visual content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo3Rbnp3dOl-"
      },
      "source": [
        "### Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "513WNgz_dOl_"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Our libraries\n",
        "#from train import train_model\n",
        "#from model_utils import *\n",
        "#from predict_utils import *\n",
        "#from vis_utils import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wh3mFGErdOmD",
        "outputId": "a1149579-035c-4000-dc19-3d730125f647"
      },
      "source": [
        "# some initial setup\n",
        "np.set_printoptions(precision=2)\n",
        "use_gpu = torch.cuda.is_available()\n",
        "np.random.seed(1234)\n",
        "if use_gpu == False:\n",
        "    print('Setup Complicated!\\nUsing CPU!')\n",
        "else:\n",
        "    print('Setup Complicated!\\nUsing GPU!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setup Complicated!\n",
            "using CPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04Dd1jTvdOmI"
      },
      "source": [
        "### Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "MXtxaVWpdOmJ",
        "outputId": "db69b845-0440-4fd1-a8ea-50f6f92d6e46"
      },
      "source": [
        "#!pip install --upgrade pip\n",
        "#!pip install kaggle\n",
        "!kaggle competitions download -c plant-pathology-2020-fgvc7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\r\n",
            "  File \"/home/nbuser/anaconda3_501/bin/kaggle\", line 5, in <module>\r\n",
            "    from kaggle.cli import main\r\n",
            "  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/kaggle/__init__.py\", line 23, in <module>\r\n",
            "    api.authenticate()\r\n",
            "  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/kaggle/api/kaggle_api_extended.py\", line 149, in authenticate\r\n",
            "    self.config_file, self.config_dir))\r\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /home/nbuser/.kaggle. Or use the environment method.\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hTBrKwIbdOmL"
      },
      "source": [
        "# DATA_DIR = \"D:/datasets/catsvsdogs/\"\n",
        "# DATA_DIR = \"D:/datasets/catsvsdogs/dev/\"\n",
        "DATA_DIR = '/media/razavi/DATA/datasets/catsvsdogs/dev/'\n",
        "sz = 224\n",
        "batch_size = 16\n",
        "\n",
        "os.listdir(DATA_DIR)\n",
        "\n",
        "trn_dir = f'{DATA_DIR}train'\n",
        "val_dir = f'{DATA_DIR}valid'\n",
        "\n",
        "os.listdir(trn_dir)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "The glob module finds all the pathnames matching a specified pattern according \n",
        "to the rules used by the Unix shell,although results are returned in arbitrary\n",
        "order.\n",
        "\"\"\"\n",
        "trn_fnames = glob.glob(f'{trn_dir}/*/*.jpg')\n",
        "trn_fnames[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5fum0fnFdOmN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ZVOTHtUudOmS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "snqSJwRUdOmU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "r9AdaSD_dOmY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "id": "PXt9UJuydOmd"
      },
      "source": [
        "## Face Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "id": "1h2QGdWMdOmd"
      },
      "source": [
        "## Object Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hd3Vw_BdOmd"
      },
      "source": [
        "<h1 align=\"center\" style=\"color:blue;\">Deep Learning (Keras)</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fqlXREMdOme"
      },
      "source": [
        "## How to install Keras\n",
        "- If you are using anaconda distribution for python, installing Keras is a very simple task.\n",
        "- Just type the following in the command prompt:\n",
        "\n",
        "### GPU version:\n",
        "<code> conda install -c anaconda keras-gpu</code>\n",
        "\n",
        "### CPU version:\n",
        "<code> conda install -c anaconda keras</code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGbXjCBcdOmf"
      },
      "source": [
        "## Tree Structure of Keras\n",
        "\n",
        "- keras Class\n",
        "\t- layers\n",
        "\t\t- Conv1D\n",
        "\t\t- Conv2D\n",
        "\t\t- Dense\n",
        "\t\t- Activation\n",
        "\t\t- Dropout\n",
        "\t- utils\n",
        "\t- datasets\n",
        "\t\t- cifar100\n",
        "\t\t- Boston housing price\n",
        "\t\t- MNIST\n",
        "\t\t- Fashion-MNIST\n",
        "\t- losses.py\n",
        "\t\t- MeanSquaredError\n",
        "\t\t- MeanAbsoluteError\n",
        "\t- metrics.py\n",
        "\t\t- accuracy\n",
        "\t\t- binary_accuracy\n",
        "\t- models.py\n",
        "\t\t- compile\n",
        "\t\t- fit\n",
        "\t\t- evaluate\n",
        "\t\t- predict\n",
        "\t- optimizers.py\n",
        "\t\t- SGD\n",
        "\t\t- Adam\n",
        "\t\t- RMSprop\n",
        "\t- activations.py\n",
        "\t\t- sigmoid()\n",
        "\t\t- tanh(x)\n",
        "\t\t- softmax(x, axis=-1)\n",
        "\t\t- relu\n",
        "\t- preprocessing\n",
        "\t\t- image\n",
        "\t\t- text\n",
        "\t\t- sequence\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDHPh-j8dOmg"
      },
      "source": [
        "### Loss function\n",
        "The currently available loss functions for Keras are as follows:\n",
        "\n",
        "- mean_squared_error\n",
        "- mean_absolute_error\n",
        "- mean_absolute_percentage_error\n",
        "- mean_squared_logarithmic_error\n",
        "- squared_hinge\n",
        "- hinge\n",
        "- categorical_hinge\n",
        "- logcosh\n",
        "- categorical_crossentropy\n",
        "- sparse_categorical_crossentropy\n",
        "- binary_crossentropy\n",
        "- kullback_leibler_divergence\n",
        "- poisson\n",
        "- cosine_proximity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbF5YRQIdOmh"
      },
      "source": [
        "## Multi-layer Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJDJKiTYdOmh"
      },
      "source": [
        "![neural_net2.jpeg](attachment:neural_net2.jpeg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SCXpMSQxdOmi"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "from keras.layers import BatchNormalization, Dropout\n",
        "\n",
        "\n",
        "\n",
        "image_size = 32\n",
        "num_channels = 3\n",
        "num_features = image_size * image_size * num_channels\n",
        "num_classes = 10\n",
        "\n",
        "num_train = 49000\n",
        "\n",
        "\n",
        "## Load CIFAR10 Dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('Train data shape: {}'.format(X_train.shape))\n",
        "print('Test  data shape: {}'.format(X_test.shape))\n",
        "\n",
        "\n",
        "## Data Visualization\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "samples_per_class = 5\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "\n",
        "for cls, name in enumerate(class_names):\n",
        "    idxs = np.flatnonzero(y_train == cls)\n",
        "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
        "    for i, idx in enumerate(idxs):\n",
        "        plt.subplot(samples_per_class, num_classes, i * num_classes + cls + 1)\n",
        "        plt.imshow(X_train[idx], interpolation='spline16')\n",
        "        plt.axis('off')\n",
        "        if i == 0:\n",
        "            plt.title(class_names[cls])\n",
        "            \n",
        "## Data Preprocessing           \n",
        "# Convert 4D arrays to 2D arrays\n",
        "X_train = X_train.reshape([-1, num_features])\n",
        "X_test  =  X_test.reshape([-1, num_features])\n",
        "\n",
        "print('Train data shape: {}'.format(X_train.shape))\n",
        "print('Test  data shape: {}'.format(X_test.shape))\n",
        "\n",
        "# convert pixel range from [0, 255] to [0., 1.]\n",
        "X_train = X_train.astype('float32')\n",
        "x_test  = X_test.astype('float32')\n",
        "\n",
        "mu = np.mean(X_train, axis=0)\n",
        "\n",
        "X_train -= mu\n",
        "X_train /= 255.0\n",
        "\n",
        "x_test -= mu\n",
        "x_test /= 255.0\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test  = keras.utils.to_categorical(y_test,  num_classes)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# First Hidden Layer\n",
        "model.add(Dense(units=100, input_shape=(num_features,)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Second Hidden Layer\n",
        "model.add(Dense(units=100))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(units=num_classes, activation='softmax'))\n",
        "\n",
        "# print model\n",
        "model.summary()\n",
        "\n",
        "## Training the model\n",
        "optimizer = keras.optimizers.Adam(lr=0.02, decay=1e-6)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(X_train[:num_train], y_train[:num_train],\n",
        "          batch_size=256,\n",
        "          epochs=15,\n",
        "          validation_data=(X_train[num_train:], y_train[num_train:]))\n",
        "\n",
        "\n",
        "\n",
        "model.save('nn-dropout-bn.h5')\n",
        "# model = keras.models.load_model('nn-dropout-bn.h5')\n",
        "\n",
        "\n",
        "model.evaluate(x_test, y_test, batch_size=256)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rbvzdINdOml"
      },
      "source": [
        "## Convolutional Neural Networks (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UA_WDZKydOml"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Fby4zgt9dOmo",
        "outputId": "945b2511-b2a4-469a-f5c7-6f2f28f46701"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "from keras.layers import BatchNormalization, Dropout\n",
        "\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
        "\n",
        "\n",
        "\n",
        "image_size = 32\n",
        "num_channels = 3\n",
        "num_features = image_size * image_size * num_channels\n",
        "num_classes = 10\n",
        "\n",
        "num_train = 49000\n",
        "\n",
        "## Load CIFAR10 Dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "\n",
        "X_train = X_train.reshape((-1, image_size, image_size, num_channels))\n",
        "X_test  =  X_test.reshape((-1, image_size, image_size, num_channels))\n",
        "## x_test  =  x_test.reshape((-1, image_size, image_size, num_channels))\n",
        "\n",
        "\n",
        "def create_cnn():\n",
        "    model = Sequential()\n",
        "\n",
        "    # Conv Block 1\n",
        "    model.add(Conv2D(64, (3, 3), padding='same', input_shape=X_train.shape[1:], activation='relu'))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    # Conv Block 2\n",
        "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    # Conv Block 3\n",
        "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    # Classifier\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = create_cnn()\n",
        "\n",
        "#print model\n",
        "model.summary()\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train[:num_train], y_train[:num_train],\n",
        "          batch_size=200,\n",
        "          epochs=1,\n",
        "          validation_data=(X_train[num_train:], y_train[num_train:]))\n",
        "\n",
        "\n",
        "\n",
        "model.evaluate(x_test, y_test, batch_size=250)\n",
        "\n",
        "\n",
        "\n",
        "y_test = np.argmax(y_test, axis=1) # to 0, 1, ..., 9\n",
        "\n",
        "plt.figure(figsize=(12, 24))\n",
        "idx = np.random.choice(len(x_test), 10, replace=False)\n",
        "\n",
        "p = model.predict(x_test[idx])\n",
        "\n",
        "for i in range(len(idx)):\n",
        "    plt.subplot(10, 2, 2*i+1)\n",
        "    plt.imshow(X_test[idx[i]], interpolation='spline16')\n",
        "    plt.title(class_names[y_test[idx[i]]])\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    \n",
        "    pred_label = np.argsort(-p[i])[:3]\n",
        "    pred_prob = [p[i][l] for l in pred_label]\n",
        "    pred_label = [class_names[l] for l in pred_label]\n",
        "    \n",
        "    plt.subplot(10, 2, 2*i+2)\n",
        "    plt.bar(range(3), pred_prob)\n",
        "    plt.xticks(range(3), pred_label)\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 30, 30, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 15, 15, 128)       73856     \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 13, 13, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 6, 6, 256)         295168    \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 4, 4, 256)         590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               131200    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,277,898\n",
            "Trainable params: 1,277,898\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Error when checking target: expected dense_2 to have shape (10,) but got array with shape (1,)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8c9100c19ec7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m           validation_data=(X_train[num_train:], y_train[num_train:]))\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_2 to have shape (10,) but got array with shape (1,)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGyG8W5qdOmr"
      },
      "source": [
        "## Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "f-VDyBfedOmr"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 256\n",
        "data_augmentation = True\n",
        "\n",
        "\n",
        "if not data_augmentation:\n",
        "    print('Training without data augmentation.')\n",
        "    model.fit(X_train[:num_train], y_train[:num_train], \n",
        "              batch_size=batch_size, \n",
        "              epochs=epochs,\n",
        "              validation_data=(X_test, y_test),\n",
        "              shuffle=True)\n",
        "else:\n",
        "    print('Training using real-time data augmentation.')\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False, \n",
        "        samplewise_center=False, \n",
        "        featurewise_std_normalization=False, \n",
        "        samplewise_std_normalization=False, \n",
        "        zca_whitening=False, \n",
        "        rotation_range=0, \n",
        "        width_shift_range=0.1, \n",
        "        height_shift_range=0.1, \n",
        "        horizontal_flip=True, \n",
        "        vertical_flip=False)\n",
        "    \n",
        "    datagen.fit(X_train[:num_train])\n",
        "    \n",
        "    model.fit_generator(datagen.flow(X_train[:num_train], y_train[:num_train], batch_size=batch_size),\n",
        "                        steps_per_epoch=num_train//batch_size,\n",
        "                        epochs=epochs,\n",
        "                        validation_data=(X_train[num_train:], y_train[num_train:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co56IJ4rdOmv"
      },
      "source": [
        "<h1 align=\"center\" style=\"color:blue;\">Telegram Bot</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "55SNJNtVdOmv"
      },
      "source": [
        "## How to install Telegram Bot API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "hidden": true,
        "id": "wmo9nQRjdOmv",
        "outputId": "7d0c29e9-ce8b-4cd1-aec2-e7f15d77e64d"
      },
      "source": [
        "!pip install pyTelegramBotAPI\n",
        "#!pip install pytelegrambotapi --upgrade\n",
        "#!pip install pip --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyTelegramBotAPI\n",
            "  Downloading pyTelegramBotAPI-3.6.7.tar.gz (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 88 kB/s  eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: requests in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from pyTelegramBotAPI) (2.22.0)\n",
            "Requirement already satisfied: six in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from pyTelegramBotAPI) (1.11.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from requests->pyTelegramBotAPI) (2.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from requests->pyTelegramBotAPI) (1.23)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from requests->pyTelegramBotAPI) (2018.10.15)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from requests->pyTelegramBotAPI) (3.0.4)\n",
            "Building wheels for collected packages: pyTelegramBotAPI\n",
            "  Building wheel for pyTelegramBotAPI (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pyTelegramBotAPI: filename=pyTelegramBotAPI-3.6.7-py3-none-any.whl size=58073 sha256=8c2c94102242e4b80b1b3d4f009f1992bb06795884d6cb685a292ef4a443cece\n",
            "  Stored in directory: /home/nbuser/.cache/pip/wheels/00/8f/a6/5302cb9bee0b25fb1cc10f509cdb088464c907d67c61481df5\n",
            "Successfully built pyTelegramBotAPI\n",
            "Installing collected packages: pyTelegramBotAPI\n",
            "Successfully installed pyTelegramBotAPI-3.6.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JN4_foXdOmy"
      },
      "source": [
        "## GET Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hFhoz8f1dOmy",
        "outputId": "78d26804-e5a1-4c7c-b3f5-e555d9700a74"
      },
      "source": [
        "import os\n",
        "import time\n",
        "from flask import Flask, request\n",
        "from telebot import types\n",
        "import telebot\n",
        "\n",
        "TOKEN = \"748794205:AAFLc6yO07DNt6xxEQe6Iv6kxY6dYbmZspk\"\n",
        "\n",
        "# https://api.telegram.org/bot748794205:AAFLc6yO07DNt6xxEQe6Iv6kxY6dYbmZspk/getUpdates\n",
        "\n",
        "\n",
        "bot = telebot.TeleBot(token=TOKEN)\n",
        "\n",
        "bot.remove_webhook()\n",
        "\n",
        "\n",
        "# Using the ReplyKeyboardMarkup class\n",
        "# It's constructor can take the following optional arguments:\n",
        "# - resize_keyboard: True/False (default False)\n",
        "# - one_time_keyboard: True/False (default False)\n",
        "# - selective: True/False (default False)\n",
        "# - row_width: integer (default 3)\n",
        "# row_width is used in combination with the add() function.\n",
        "# It defines how many buttons are fit on each row before continuing on the next row.\n",
        "\n",
        "\n",
        "@bot.message_handler(commands=['start'])\n",
        "def start(message):\n",
        "    markup = types.ReplyKeyboardMarkup(row_width=2)\n",
        "    itembtn1 = types.KeyboardButton('a')\n",
        "    itembtn2 = types.KeyboardButton('v')\n",
        "    itembtn3 = types.KeyboardButton('d')\n",
        "    markup.add(itembtn1, itembtn2, itembtn3)\n",
        "    bot.send_message(message.chat.id, \"Choose one letter:\", reply_markup=markup)\n",
        "\n",
        "\n",
        "@bot.message_handler(commands=['help'])\n",
        "def start(message):\n",
        "    bot.reply_to(message, \"I will do sth\")\n",
        "\n",
        "\n",
        "@bot.message_handler(commands=['color2grayscale'])\n",
        "def send_photo(message):\n",
        "    bot.send_chat_action(message.chat.id, 'upload_photo')\n",
        "    img = open('out.png', 'rb')\n",
        "    bot.send_photo(message.chat.id, img, reply_to_message_id=message.message_id)\n",
        "    img.close()\n",
        "\n",
        "\n",
        "@bot.message_handler(content_types=['photo'])\n",
        "def handle_photo(message):\n",
        "    print('message.photo =', message.photo)\n",
        "    fileID = message.photo[-1].file_id\n",
        "    print('fileID =', fileID)\n",
        "    file_info = bot.get_file(fileID)\n",
        "    print('file.file_path =', file_info.file_path)\n",
        "    downloaded_file = bot.download_file(file_info.file_path)\n",
        "\n",
        "    with open(\"AAA.jpg\", 'wb') as new_file:\n",
        "        new_file.write(downloaded_file)\n",
        "\n",
        "    markup = types.ReplyKeyboardMarkup(row_width=3)\n",
        "    itembtn1 = types.KeyboardButton('color2grayscale')\n",
        "    itembtn2 = types.KeyboardButton('change size')\n",
        "    itembtn3 = types.KeyboardButton('menu')\n",
        "    markup.add(itembtn1, itembtn2, itembtn3)\n",
        "    bot.send_message(message.chat.id, \"what can I do for you?\", reply_markup=markup)\n",
        "\n",
        "\n",
        "# Handles all sent documents and audio files\n",
        "@bot.message_handler(content_types=['document', 'audio'])\n",
        "def handle_docs_audio():\n",
        "    pass\n",
        "\n",
        "\n",
        "# Handles all text messages that match the regular expression\n",
        "@bot.message_handler(regexp=\"SOME_REGEXP\")\n",
        "def handle_message(message):\n",
        "    pass\n",
        "\n",
        "\n",
        "# Handles all messages for which the lambda returns True\n",
        "@bot.message_handler(func=lambda message: message.document.mime_type == 'text/plain', content_types=['document'])\n",
        "def handle_text_doc(message):\n",
        "    pass\n",
        "\n",
        "\n",
        "# Which could also be defined as:\n",
        "def test_message(message):\n",
        "    return message.document.mime_type == 'text/plain'\n",
        "\n",
        "\n",
        "@bot.message_handler(func=test_message, content_types=['document'])\n",
        "def handle_text_doc(message):\n",
        "    pass\n",
        "\n",
        "\n",
        "# Handlers can be stacked to create a function which will be called if either message_handler is eligible\n",
        "# This handler will be called if the message starts with '/hello' OR is some emoji\n",
        "@bot.message_handler(commands=['hello'])\n",
        "@bot.message_handler(func=lambda msg: msg.text.encode(\"utf-8\") == SOME_FANCY_EMOJI)\n",
        "def send_something(message):\n",
        "    pass\n",
        "\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        bot.polling()\n",
        "    except Exception:\n",
        "        time.sleep(15)\n",
        "\n",
        "\"\"\"\n",
        "# Upon calling this function, TeleBot starts polling the Telegram servers for new messages.\n",
        "# - none_stop: True/False (default False) - Don't stop polling when receiving an error from the Telegram servers\n",
        "# - interval: True/False (default False) - The interval between polling requests\n",
        "#           Note: Editing this parameter harms the bot's response time\n",
        "# - timeout: integer (default 20) - Timeout in seconds for long polling.\n",
        "tb.polling(none_stop=False, interval=0, timeout=20)\n",
        "\n",
        "# getMe\n",
        "user = bot.get_me()\n",
        "\n",
        "# setWebhook\n",
        "tb.set_webhook(url=\"http://example.com\", certificate=open('mycert.pem'))\n",
        "# unset webhook\n",
        "tb.remove_webhook()\n",
        "\n",
        "# getUpdates\n",
        "updates = tb.get_updates()\n",
        "updates = tb.get_updates(1234,100,20) #get_Updates(offset, limit, timeout):\n",
        "\n",
        "# sendMessage\n",
        "tb.send_message(chat_id, text)\n",
        "\n",
        "# forwardMessage\n",
        "tb.forward_message(to_chat_id, from_chat_id, message_id)\n",
        "\n",
        "# All send_xyz functions which can take a file as an argument, can also take a file_id instead of a file.\n",
        "# sendPhoto\n",
        "photo = open('/tmp/photo.png', 'rb')\n",
        "tb.send_photo(chat_id, photo)\n",
        "tb.send_photo(chat_id, \"FILEID\")\n",
        "\n",
        "# sendAudio\n",
        "audio = open('/tmp/audio.mp3', 'rb')\n",
        "tb.send_audio(chat_id, audio)\n",
        "tb.send_audio(chat_id, \"FILEID\")\n",
        "\n",
        "## sendAudio with duration, performer and title.\n",
        "tb.send_audio(CHAT_ID, file_data, 1, 'eternnoir', 'pyTelegram')\n",
        "\n",
        "# sendVoice\n",
        "voice = open('/tmp/voice.ogg', 'rb')\n",
        "tb.send_voice(chat_id, voice)\n",
        "tb.send_voice(chat_id, \"FILEID\")\n",
        "\n",
        "# sendDocument\n",
        "doc = open('/tmp/file.txt', 'rb')\n",
        "tb.send_document(chat_id, doc)\n",
        "tb.send_document(chat_id, \"FILEID\")\n",
        "\n",
        "# sendSticker\n",
        "sti = open('/tmp/sti.webp', 'rb')\n",
        "tb.send_sticker(chat_id, sti)\n",
        "tb.send_sticker(chat_id, \"FILEID\")\n",
        "\n",
        "# sendVideo\n",
        "video = open('/tmp/video.mp4', 'rb')\n",
        "tb.send_video(chat_id, video)\n",
        "tb.send_video(chat_id, \"FILEID\")\n",
        "\n",
        "# sendVideoNote\n",
        "videonote = open('/tmp/videonote.mp4', 'rb')\n",
        "tb.send_video_note(chat_id, videonote)\n",
        "tb.send_video_note(chat_id, \"FILEID\")\n",
        "\n",
        "# sendLocation\n",
        "tb.send_location(chat_id, lat, lon)\n",
        "\n",
        "# sendChatAction\n",
        "# action_string can be one of the following strings: 'typing', 'upload_photo', 'record_video', 'upload_video',\n",
        "# 'record_audio', 'upload_audio', 'upload_document' or 'find_location'.\n",
        "tb.send_chat_action(chat_id, action_string)\n",
        "\n",
        "# getFile\n",
        "# Downloading a file is straightforward\n",
        "# Returns a File object\n",
        "import requests\n",
        "file_info = tb.get_file(file_id)\n",
        "\n",
        "file = requests.get('https://api.telegram.org/file/bot{0}/{1}'.format(API_TOKEN, file_info.file_path))\n",
        "\n",
        "\n",
        "\n",
        "@bot.message_handler(func=lambda message: True, content_types=['text'])\n",
        "def echo_message(message):\n",
        "    bot.reply_to(message, message.text)\n",
        "\n",
        "\n",
        "@server.route('/' + TOKEN, methods=['POST'])\n",
        "def getMessage():\n",
        "    bot.process_new_updates([telebot.types.Update.de_json(request.stream.read().decode(\"utf-8\"))])\n",
        "    return \"!\", 200\n",
        "\n",
        "\n",
        "@server.route(\"/\")\n",
        "def webhook():\n",
        "    bot.remove_webhook()\n",
        "    bot.set_webhook(url='https://mytelegrambot-saeed.herokuapp.com/' + TOKEN)\n",
        "    return \"!\", 200\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    server.run(host=\"0.0.0.0\", port=int(os.environ.get('PORT', 5000)))\n",
        "\t\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-06 01:45:31,266 (util.py:66 PollingThread) ERROR - TeleBot: \"ApiException occurred, args=('A request to the Telegram API was unsuccessful. The server returned HTTP 409 Conflict. Response body:\\n[b\\'{\"ok\":false,\"error_code\":409,\"description\":\"Conflict: terminated by other getUpdates request; make sure that only one bot instance is running\"}\\']',)\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/telebot/util.py\", line 60, in run\n",
            "    task(*args, **kwargs)\n",
            "  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/telebot/__init__.py\", line 279, in __retrieve_updates\n",
            "    updates = self.get_updates(offset=(self.last_update_id + 1), timeout=timeout)\n",
            "  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/telebot/__init__.py\", line 249, in get_updates\n",
            "    json_updates = apihelper.get_updates(self.token, offset, limit, timeout, allowed_updates)\n",
            "  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/telebot/apihelper.py\", line 194, in get_updates\n",
            "    return _make_request(token, method_url, params=payload)\n",
            "  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/telebot/apihelper.py\", line 60, in _make_request\n",
            "    return _check_result(method_name, result)['result']\n",
            "  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/telebot/apihelper.py\", line 79, in _check_result\n",
            "    raise ApiException(msg, method_name, result)\n",
            "telebot.apihelper.ApiException: A request to the Telegram API was unsuccessful. The server returned HTTP 409 Conflict. Response body:\n",
            "[b'{\"ok\":false,\"error_code\":409,\"description\":\"Conflict: terminated by other getUpdates request; make sure that only one bot instance is running\"}']\n",
            "\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkbTwEGvdOm1"
      },
      "source": [
        "## WEBHOOK Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztjctMWidOm1"
      },
      "source": [
        "<h1 align=\"center\" style=\"color:blue;\">Block chain</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kcIjPCqXdOm2",
        "outputId": "51c11208-9d96-45f9-9c52-d22fbba801d0"
      },
      "source": [
        "import hashlib\n",
        "\n",
        "hashlib.sha256('hi'.encode()).hexdigest()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'8f434346648f6b96df89dda901c5176b10a6d83961dd3c1ac88b59b2dc327aa4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "giTNya2NdOm4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}